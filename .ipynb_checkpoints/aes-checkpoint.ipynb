{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries and data.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "essay_data = pd.read_csv(\"domain1.csv\")\n",
    "print(essay_data.columns)\n",
    "print(len(essay_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing, extracting the features.\n",
    "def somefunc(X,target_col):\n",
    "    #feature_columns = [\"essay\",\"word_count\",\"sentence_count\",\"wrong_words\",\"spelling_mistakes\",\"no_of_domain_words\",\"word_to_sent_ratio\",\"num_of_characters\",'NN','NNP','VBZ','NNPS','NNS','IN','PRP','VB','JJ','VBP','VBG',\"target\"]\n",
    "    feature_columns = [\"essay\",\"word_count\",\"long_word_count\",\"avg_word_length_per_essay\",\"wrong_words\",\"no_of_domain_words\",\"word_to_sent_ratio\",\"num_of_characters\",\"sentence_count\",\"noun_count\",\"verb_count\",\"comma_count\",\"punctuation_count\",\"adjective_count\",\"adverb_count\",\"quotation_mark_count\",\"spelling_mistakes\",\"target\"]\n",
    "    feature_pd = pd.DataFrame(index = X.index, columns = feature_columns)\n",
    "    feature_pd['essay'] = X['essay']\n",
    "    feature_pd['target'] = X[target_col]\n",
    "    return feature_pd\n",
    "feature_data = somefunc(essay_data,'domain1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "feature_cols = list(feature_data.columns[:-1])\n",
    "X_all = feature_data[feature_cols]\n",
    "y_all = feature_data['target'] #domain1_score is equal to sum of rater_domain1 and rater_domain2, so that columns are not needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing.\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import grammar_check\n",
    "from collections import Counter\n",
    "import textmining\n",
    "from time import time\n",
    "\n",
    "def featureSet(X): #X would be X_train and X_test\n",
    "    #WordCount\n",
    "    for index,row in X.iterrows():\n",
    "        #Add the sentence count\n",
    "        text = unicode(row['essay'],errors='ignore') \n",
    "        text = \" \".join(filter(lambda x:x[0]!='@', text.split())) #To remove proper nouns tagged in the data-set which may result into false positives during POS tagging.\n",
    "        \n",
    "        tokenized_essay = nltk.sent_tokenize(text)\n",
    "        sent_count = len(tokenized_essay)\n",
    "        row['sentence_count'] = sent_count\n",
    "        \n",
    "        #Add word count after removing the stop words.\n",
    "        words = nltk.word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n",
    "        \n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                words.remove(word)\n",
    "        word_count = len(words)\n",
    "        \n",
    "        row['word_count'] = word_count\n",
    "        row['word_to_sent_ratio'] = round(float(word_count/float(sent_count)),2)\n",
    "        row['num_of_characters'] = nltk.FreqDist(text).N()\n",
    "\n",
    "        tool = grammar_check.LanguageTool('en-US')\n",
    "        matches = tool.check(text)\n",
    "        row['spelling_mistakes'] = len(matches)\n",
    "        \n",
    "        #No_of_domain_words after removing the stop words and punctuations from the essay.\n",
    "        cnt = 0\n",
    "        wrong_word_count = 0\n",
    "        for word in words:\n",
    "            if wn.synsets(word):\n",
    "                cnt += 1\n",
    "            else:\n",
    "                wrong_word_count += 1\n",
    "        row['no_of_domain_words'] = cnt\n",
    "        row['wrong_words'] = wrong_word_count\n",
    "        \n",
    "        \n",
    "        #POS TAGS\n",
    "        count= Counter([j for i,j in nltk.pos_tag(words)])\n",
    "        pos_list = ['NN','NNP','VBZ','NNPS','NNS','IN','PRP','VB','JJ','VBP','VBG']\n",
    "        \n",
    "        for i in pos_list:\n",
    "            row[i] = count[i]\n",
    "        \n",
    "\n",
    "'''start = time()        \n",
    "featureSet(X_train)\n",
    "end = time()\n",
    "print (\"Generated the features for training data in {:.4f} minutes\".format((end - start)/60.0))\n",
    "\n",
    "start = time()\n",
    "featureSet(X_test)\n",
    "end = time()\n",
    "print (\"Generated the features for testing data in {:.4f} minutes\".format((end - start)/60.0))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import grammar_check\n",
    "from collections import Counter\n",
    "import textmining\n",
    "from time import time\n",
    "\n",
    "def featureSet2(X): #X would be X_train and X_test\n",
    "    #WordCount\n",
    "    for index,row in X.iterrows():\n",
    "        \n",
    "        text = unicode(row['essay'],errors='ignore') \n",
    "        text = \" \".join(filter(lambda x:x[0]!='@', text.split())) #To remove proper nouns tagged in the data-set which may result into false positives during POS tagging.\n",
    "        \n",
    "        punctuation = ['.','?', '!', ':', ';']\n",
    "        #Comma count\n",
    "        comma_count = text.count(',')\n",
    "        row['comma_count'] = comma_count\n",
    "        \n",
    "        #Punctuation count\n",
    "        punctuation_count = 0\n",
    "        for punct in punctuation:\n",
    "            punctuation_count += text.count(punct)\n",
    "        row['punctuation_count'] = punctuation_count\n",
    "        \n",
    "        #Quotation marks count\n",
    "        quotation_mark_count = text.count('\"')\n",
    "        quotation_mark_count += text.count(\"'\")\n",
    "        row['quotation_mark_count'] = quotation_mark_count\n",
    "        \n",
    "        #Add the sentence count\n",
    "               \n",
    "        tokenized_essay = nltk.sent_tokenize(text)\n",
    "        sent_count = len(tokenized_essay)\n",
    "        row['sentence_count'] = sent_count\n",
    "        \n",
    "        #Add word count after removing the stop words.\n",
    "        words = nltk.word_tokenize(text)\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation \n",
    "        \n",
    "        for word in words:\n",
    "            if word in stop_words:\n",
    "                words.remove(word)\n",
    "        word_count = len(words)\n",
    "        \n",
    "        row['word_count'] = word_count\n",
    "        \n",
    "        #Long word count\n",
    "        long_word_count = 0\n",
    "        total_word_length = 0\n",
    "        for word in words:\n",
    "            total_word_length += len(word)\n",
    "            if len(word) > 6:\n",
    "                long_word_count +=1\n",
    "        row['long_word_count'] = long_word_count\n",
    "        \n",
    "        #Average word length per essay\n",
    "        row['avg_word_length_per_essay'] = round((total_word_length/float(len(words))),2)\n",
    "        \n",
    "        \n",
    "        tool = grammar_check.LanguageTool('en-US')\n",
    "        matches = tool.check(text)\n",
    "        row['spelling_mistakes'] = len(matches)\n",
    "           \n",
    "        #POS TAGS\n",
    "        count= Counter([j for i,j in nltk.pos_tag(words)])\n",
    "               \n",
    "        row['noun_count'] = count['NN'] + count['NNS'] + count['NNPS'] + count['NNP']\n",
    "        row['verb_count'] = count['VB'] + count['VBG'] + count['VBP'] + count['VBN'] + count['VBZ']\n",
    "        row['adjective_count'] = count['JJ'] + count['JJR'] \n",
    "        row['adverb_count'] = count['RB'] + count['RBR'] + count['RBS']\n",
    "        \n",
    "        #No_of_domain_words and wrong words after removing the stop words and punctuations from the essay.\n",
    "        cnt = 0\n",
    "        wrong_word_count = 0\n",
    "        for word in words:\n",
    "            if wn.synsets(word):\n",
    "                cnt += 1\n",
    "            else:\n",
    "                wrong_word_count += 1\n",
    "        row['no_of_domain_words'] = cnt\n",
    "        row['wrong_words'] = wrong_word_count        \n",
    "        \n",
    "        #Word to sentence ratio\n",
    "        row['word_to_sent_ratio'] = round(float(word_count/float(sent_count)),2)\n",
    "        \n",
    "        #Number of characters\n",
    "        row['num_of_characters'] = nltk.FreqDist(text).N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generate the feature set.\n",
    "def GenerateFeatures(X):\n",
    "    start = time()\n",
    "    featureSet2(X)\n",
    "    end = time()\n",
    "    print (\"Generated the features for the entire data-set in {:.4f} minutes\".format((end - start)/60.0))\n",
    "\n",
    "GenerateFeatures(X_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_all.to_csv('features_set_1.csv', sep='\\t')\n",
    "X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fitting, predicting and calculating error. \n",
    "#Using LinearRegression, 5 fold cross validation and quadratic kappa as an error metric.\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from metrics import kappa\n",
    "\n",
    "def Evaluate(feature_list):\n",
    "    model = LinearRegression()\n",
    "\n",
    "    #Simple K-Fold cross validation. 5 folds.\n",
    "    cv = cross_validation.KFold(len(X_all), n_folds=5,shuffle=True)\n",
    "    results = []\n",
    "    \n",
    "    for traincv, testcv in cv:\n",
    "            X_test, X_train, y_test, y_train = X_all.ix[testcv], X_all.ix[traincv], y_all.ix[testcv], y_all.ix[traincv]\n",
    "\n",
    "            #final_train_data = X_train.drop('essay',axis = 1)\n",
    "            #final_test_data = X_test.drop('essay',axis = 1)\n",
    "            \n",
    "            final_train_data = X_train[feature_list]\n",
    "            final_test_data = X_test[feature_list]\n",
    "            \n",
    "            model.fit(final_train_data,y_train)\n",
    "            start = time()\n",
    "            y_pred = model.predict(final_test_data)\n",
    "            end = time()\n",
    "\n",
    "            #print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "            result = kappa(y_test.values,y_pred,weights='quadratic')\n",
    "            results.append(result)\n",
    "            \n",
    "            #probas = model.fit(train[traincv], target[traincv]).predict_proba(train[testcv])\n",
    "\n",
    "\n",
    "    #print \"Results: \" + str( np.array(results).mean() )\n",
    "    return str(np.array(results).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = [\"word_count\",\"long_word_count\",\"avg_word_length_per_essay\",\"wrong_words\",\"no_of_domain_words\",\"word_to_sent_ratio\",\"num_of_characters\",\"sentence_count\",\"noun_count\",\"verb_count\",\"comma_count\",\"punctuation_count\",\"adjective_count\",\"adverb_count\",\"quotation_mark_count\",\"spelling_mistakes\"]\n",
    "feature_dict = {}\n",
    "for i in feature_columns:\n",
    "    feature_dict[i] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in feature_columns:\n",
    "    score_ = Evaluate([f])\n",
    "    feature_dict[f] = round(float(score)*100,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Single feature Kappa\n",
    "import operator\n",
    "sorted_feature_list = sorted(feature_dict.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Forward feature selection.\n",
    "sorted_f = [i[0] for i in sorted_feature_list]\n",
    "for i in range(1,len(sorted_f)+1):\n",
    "    forward_feature_list = sorted_f[:i]\n",
    "    print forward_feature_list, Evaluate(forward_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.680805858355\n",
      "0.69033122005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wrong_words',\n",
       " 'verb_count',\n",
       " 'punctuation_count',\n",
       " 'adverb_count',\n",
       " 'quotation_mark_count',\n",
       " 'adjective_count',\n",
       " 'spelling_mistakes',\n",
       " 'sentence_count',\n",
       " 'avg_word_length_per_essay',\n",
       " 'no_of_domain_words',\n",
       " 'word_to_sent_ratio']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_indexes  = [0,1,2,3,5,8,10,11,13,14,15] #Do not delete\n",
    "selected_features = [sorted_f[i] for i in selected_indexes]\n",
    "print Evaluate(selected_features)\n",
    "print Evaluate(sorted_f)\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>long_word_count</th>\n",
       "      <th>avg_word_length_per_essay</th>\n",
       "      <th>wrong_words</th>\n",
       "      <th>no_of_domain_words</th>\n",
       "      <th>word_to_sent_ratio</th>\n",
       "      <th>num_of_characters</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>comma_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>quotation_mark_count</th>\n",
       "      <th>spelling_mistakes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427.00</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "      <td>1427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>374</td>\n",
       "      <td>140</td>\n",
       "      <td>177.00</td>\n",
       "      <td>114</td>\n",
       "      <td>306</td>\n",
       "      <td>725.0</td>\n",
       "      <td>1040</td>\n",
       "      <td>49</td>\n",
       "      <td>160</td>\n",
       "      <td>97</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>249</td>\n",
       "      <td>59</td>\n",
       "      <td>4.86</td>\n",
       "      <td>46</td>\n",
       "      <td>199</td>\n",
       "      <td>14.0</td>\n",
       "      <td>239</td>\n",
       "      <td>23</td>\n",
       "      <td>81</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>23.00</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7</td>\n",
       "      <td>87</td>\n",
       "      <td>33</td>\n",
       "      <td>50</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>241</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word_count  long_word_count  avg_word_length_per_essay  wrong_words  \\\n",
       "count         1427             1427                    1427.00         1427   \n",
       "unique         374              140                     177.00          114   \n",
       "top            249               59                       4.86           46   \n",
       "freq            14               28                      23.00           38   \n",
       "\n",
       "        no_of_domain_words  word_to_sent_ratio  num_of_characters  \\\n",
       "count                 1427              1427.0               1427   \n",
       "unique                 306               725.0               1040   \n",
       "top                    199                14.0                239   \n",
       "freq                    18                13.0                  7   \n",
       "\n",
       "        sentence_count  noun_count  verb_count  comma_count  \\\n",
       "count             1427        1427        1427         1427   \n",
       "unique              49         160          97           48   \n",
       "top                 23          81          52            2   \n",
       "freq                87          33          50           83   \n",
       "\n",
       "        punctuation_count  adjective_count  adverb_count  \\\n",
       "count                1427             1427          1427   \n",
       "unique                 53               59            47   \n",
       "top                    24               24            19   \n",
       "freq                   65               62            71   \n",
       "\n",
       "        quotation_mark_count  spelling_mistakes  \n",
       "count                   1427               1427  \n",
       "unique                    26                 26  \n",
       "top                        0                  2  \n",
       "freq                     241                187  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_train_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Redundant code\n",
    "'''\n",
    "#Training testing using train_test_split\n",
    "\n",
    "num_train = 1450\n",
    "\n",
    "# Set the number of testing points\n",
    "num_test = X_all.shape[0] - num_train\n",
    "\n",
    "\n",
    "#Shuffle and split the dataset into the number of training and testing points above\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=num_train,random_state=42)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "print type(X_test)\n",
    "print type(y_test)\n",
    "y_test.name = \"scores\"\n",
    "\n",
    "\n",
    "def termdocumentmatrix():\n",
    "    # Initialize class to create term-document matrix\n",
    "    tdm = textmining.TermDocumentMatrix()\n",
    "    # Add the documents\n",
    "    for index,row in essay_data.iterrows():\n",
    "        tdm.add_doc(row['essay'])\n",
    "    # Write out the matrix to a csv file. Note that setting cutoff=2 means\n",
    "    # that words which appear in 1 or more documents will be included in\n",
    "    # the output (i.e. every word will appear in the output). The default\n",
    "    # for cutoff is 2, since we usually aren't interested in words which\n",
    "    # appear in a single document. For this example we want to see all\n",
    "    # words however, hence cutoff=1.\n",
    "    tdm.write_csv('matrix.csv', cutoff=1)\n",
    "    # Instead of writing out the matrix you can also access its rows directly.\n",
    "    # Let's print them to the screen.\n",
    "    return tdm\n",
    "    \n",
    "def domainInformationContent(X,Y):\n",
    "    #Get essay with maximum score.\n",
    "    essay_index_with_max_score = Y.idxmax()\n",
    "    essay_with_max_score = X.ix[essay_index_with_max_score]['essay']\n",
    "    #print essay_with_max_score\n",
    "    \n",
    "    #Extract the noun words from this essay and feed it to WordNet.\n",
    "    lines = 'lines is some string of words'\n",
    "    # function to test if something is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # do the nlp stuff\n",
    "    tokenized_max = nltk.word_tokenize(essay_with_max_score)\n",
    "    nouns_max = [word for (word, pos) in nltk.pos_tag(tokenized_max) if is_noun(pos)] \n",
    "    return nouns_max\n",
    "      \n",
    "\n",
    "\n",
    "#Some random plots\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "%matplotlib inline \n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "plt.plot(X_test['no_of_domain_words'],y_test,'.')\n",
    "plt.plot(X_test['no_of_domain_words'],y_pred,'-')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Feature wise training\n",
    "def featurewisetraining(feature):\n",
    "    _1dfeature = X_train[feature].reshape(-1,1)\n",
    "    lr.fit(_1dfeature,y_train)\n",
    "    X_T = X_test[feature].reshape(-1,1)\n",
    "    print \"Score for feature \",feature,\" is \",lr.score(X_T,y_test)\n",
    "    plt.plot(X_test[feature],y_test,'.')\n",
    "    plt.plot(X_test[feature],y_pred,'-')\n",
    "    plt.show()\n",
    "    \n",
    "featurewisetraining('word_count')\n",
    "featurewisetraining('sentence_count')\n",
    "featurewisetraining('spelling_mistakes')\n",
    "featurewisetraining('no_of_domain_words')\n",
    "featurewisetraining('word_to_sent_ratio')\n",
    "featurewisetraining('wrong_words')\n",
    "featurewisetraining('num_of_characters')\n",
    "\n",
    "#Hence we drop the word_to_sent ratio feature.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
